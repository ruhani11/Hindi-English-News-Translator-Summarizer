{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d31793f-4ba2-49b2-a125-04e978770fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: streamlit in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (1.37.1)\n",
      "Requirement already satisfied: torch in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (2.3.1+cpu)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (10.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (5.29.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (16.1.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\ruhan\\appdata\\roaming\\python\\python312\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<5,>=2.1.5 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ruhan\\appdata\\roaming\\python\\python312\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ruhan\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ruhan\\appdata\\roaming\\python\\python312\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 transformers streamlit torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f537777-2c69-4131-b5b6-4c25a30905c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (2.3.1+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (0.18.1+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (2.3.1+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ruhan\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385feea3-b63f-4bd6-bb11-fd064e0a0706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruhan\\anaconda3\\envs\\tf_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33a82a1b-0903-44c5-934a-a8aba6e12c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, MarianMTModel, MarianTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea7201f-fde6-4aa9-809b-f167ef4a3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_articles(url):\n",
    "    \"\"\"\n",
    "    Scrapes the title, date, and main content from a Hindi news article.\n",
    "    Adjust the tags/classes as per the site structure.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Adjust selectors for the specific site structure\n",
    "    title = soup.find('h1')\n",
    "    date = soup.find('span')\n",
    "    content_div = soup.find('div')\n",
    "\n",
    "    title = title.text.strip() if title else \"Title not found\"\n",
    "    date = date.text.strip() if date else \"Date not found\"\n",
    "    content = content_div.get_text(separator=' ', strip=True) if content_div else \"Content not found\"\n",
    "\n",
    "    return {'title': title, 'date': date, 'content': content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846479de-3892-4c73-b72c-333e7e8f6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "@st.cache_resource\n",
    "def load_translation_model():\n",
    "    tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-hi-en\")\n",
    "    model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-hi-en\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def translate_text(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0bd4ed-6b63-447a-8205-2a4b4f3d9964",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def load_summarization_model():\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "    return tokenizer, model\n",
    "\n",
    "def summarize_text(text, tokenizer, model):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "368700c8-e0d7-4d7c-800a-05d4e7a31650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruhan\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load and clean dataset\n",
    "df = pd.read_csv('scrapped_clean.csv')  # ✅ use read_csv for CSV files\n",
    "df.columns = [\"hindi\", \"english\"]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154fb24c-2241-4f8c-9a91-905b26e5350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hindi': 'पहलगाम अटैक पर विवादित पोस्ट, 7 राज्यों में 26 गिरफ्तार:इनमें विधायक, पत्रकार, वकील और स्टूडेंट शामिल; देश विरोधी टिप्पणी की थी', 'english': 'Disputed posts on Pahalgam attack, 26 arrested in 7 states: these include MLAs, journalists, lawyers and students; Was made anti -national comments'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845c3d65-67e9-4317-9f3f-e0d02164dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 239/239 [00:00<00:00, 6196.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Format dataset to match Hugging Face translation format\n",
    "def format_translation(example):\n",
    "    return {\n",
    "        \"translation\": {\n",
    "            \"hi\": example[\"hindi\"],\n",
    "            \"en\": example[\"english\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "formatted_dataset = dataset.map(format_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7178a2a4-3b6e-4e14-bfdb-97eecbb6967f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruhan\\anaconda3\\envs\\tf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ruhan\\anaconda3\\envs\\tf_env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map: 100%|██████████| 239/239 [00:00<00:00, 922.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    hi_texts = [item[\"hi\"] for item in batch[\"translation\"]]\n",
    "    en_texts = [item[\"en\"] for item in batch[\"translation\"]]\n",
    "\n",
    "    inputs = tokenizer(hi_texts, padding=\"max_length\", truncation=True, max_length=64)\n",
    "    targets = tokenizer(en_texts, padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": targets[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(tokenize, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875081a3-7cd1-45ae-ac82-65f1c479d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hindi': 'पहलगाम अटैक पर विवादित पोस्ट, 7 राज्यों में 26 गिरफ्तार:इनमें विधायक, पत्रकार, वकील और स्टूडेंट शामिल; देश विरोधी टिप्पणी की थी', 'english': 'Disputed posts on Pahalgam attack, 26 arrested in 7 states: these include MLAs, journalists, lawyers and students; Was made anti -national comments', 'translation': {'en': 'Disputed posts on Pahalgam attack, 26 arrested in 7 states: these include MLAs, journalists, lawyers and students; Was made anti -national comments', 'hi': 'पहलगाम अटैक पर विवादित पोस्ट, 7 राज्यों में 26 गिरफ्तार:इनमें विधायक, पत्रकार, वकील और स्टूडेंट शामिल; देश विरोधी टिप्पणी की थी'}, 'input_ids': [6567, 4017, 975, 1067, 23881, 395, 33, 7419, 602, 11948, 2, 952, 11747, 12, 1998, 5876, 24, 41741, 40228, 28143, 2, 20322, 2, 12931, 7, 60843, 1204, 41, 512, 6373, 6128, 15, 167, 0, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126, 61126], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [9599, 11436, 5435, 7000, 23, 70, 9121, 15728, 1038, 4811, 145, 5368, 8479, 2, 1998, 50, 2735, 578, 23, 5435, 19, 952, 2092, 23, 24, 290, 50, 41701, 1989, 1445, 711, 23, 2, 34888, 3313, 4417, 11840, 23, 2, 10660, 1884, 667, 1579, 10, 50, 1081, 1383, 7749, 25519, 41, 2572, 1718, 243, 138, 4957, 18, 507, 2062, 860, 10227, 10984, 0, 61126, 61126]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d2a6887-5ded-4e6e-998d-e0e47ab04619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hindi', 'english', 'translation', 'input_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c491560b-ab8b-43a3-95a3-6e5a1468c00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_dataset[0]['input_ids']))  # Should be 64 if you used max_length=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b1556d4-831d-4b2b-91b7-58446ef43712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruhan\\anaconda3\\envs\\tf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ruhan\\anaconda3\\envs\\tf_env\\lib\\site-packages\\accelerate\\accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-hi-en\"\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d86f90-8225-484a-88a4-a4a72fefa97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Improved prediction function that accepts a model\n",
    "def get_predictions_with_model(df, model, batch_size=8):\n",
    "    print(\"Generating predictions in batches...\")\n",
    "    preds = []\n",
    "    hindi_texts = df[\"hindi\"].tolist()\n",
    "\n",
    "    for i in tqdm(range(0, len(hindi_texts), batch_size)):\n",
    "        batch_texts = hindi_texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "        with torch.no_grad():\n",
    "            translated = model.generate(**inputs)\n",
    "        batch_preds = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        preds.extend(batch_preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cb0bbe5-2130-4b32-8a3e-c1b4bc3fded9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Evaluating BLEU before fine-tuning...\n",
      "Generating predictions in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 30/30 [07:11<00:00, 14.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BLEU score before fine-tuning: {'score': 12.013937343727523, 'counts': [2550, 967, 426, 199], 'totals': [5946, 5707, 5468, 5229], 'precisions': [42.88597376387487, 16.94410373225863, 7.790782735918069, 3.805698986421878], 'bp': 0.9916262424042355, 'sys_len': 5946, 'ref_len': 5996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "metric = load(\"sacrebleu\")\n",
    "\n",
    "# Load original base model again for pre-fine-tuning eval\n",
    "base_model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "print(\"🧪 Evaluating BLEU before fine-tuning...\")\n",
    "baseline_preds = get_predictions_with_model(df, base_model)\n",
    "bleu_before = metric.compute(predictions=baseline_preds, references=[[ref] for ref in df[\"english\"]])\n",
    "print(\"✅ BLEU score before fine-tuning:\", bleu_before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19903c9e-c454-4e94-91f8-3250c4abc5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruhan\\anaconda3\\envs\\tf_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 09:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.177000</td>\n",
       "      <td>3.375665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.423900</td>\n",
       "      <td>2.674280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.942100</td>\n",
       "      <td>2.278084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.396900</td>\n",
       "      <td>2.050273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.504200</td>\n",
       "      <td>1.977165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=3.213486576080322, metrics={'train_runtime': 601.3336, 'train_samples_per_second': 1.987, 'train_steps_per_second': 0.499, 'total_flos': 20254273044480.0, 'train_loss': 3.213486576080322, 'epoch': 5.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6e15663-57c4-458c-8c73-062e6cb775b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Evaluating BLEU after fine-tuning...\n",
      "Generating predictions in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 30/30 [06:10<00:00, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BLEU score after fine-tuning: {'score': 26.06905981426099, 'counts': [3206, 1801, 1108, 724], 'totals': [5932, 5693, 5454, 5215], 'precisions': [54.045853000674306, 31.63534164763745, 20.315364869820314, 13.883029721955896], 'bp': 0.9892690505480524, 'sys_len': 5932, 'ref_len': 5996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🧪 Evaluating BLEU after fine-tuning...\")\n",
    "fine_tuned_preds = get_predictions_with_model(df, model)  # 'model' is fine-tuned now\n",
    "bleu_after = metric.compute(predictions=fine_tuned_preds, references=[[ref] for ref in df[\"english\"]])\n",
    "print(\"✅ BLEU score after fine-tuning:\", bleu_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f70304b9-119d-4a98-aa29-3d04d147948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_news_summary(article_data, translated_title, translated_text, summary):\n",
    "    \"\"\"\n",
    "    Display results on the Streamlit dashboard.\n",
    "    \"\"\"\n",
    "    st.title(\"📰 Hindi News Summarizer & Translator\")\n",
    "\n",
    "    st.subheader(\"📝 Title (Hindi):\")\n",
    "    st.write(article_data['title'])\n",
    "\n",
    "    st.subheader(\"🌐 Title (English):\")\n",
    "    st.write(translated_title)\n",
    "\n",
    "    st.subheader(\"📅 Published Date:\")\n",
    "    st.write(article_data['date'])\n",
    "\n",
    "    st.subheader(\"🗞️ Full Article (Hindi):\")\n",
    "    st.write(article_data['content'])\n",
    "\n",
    "    st.subheader(\"🌍 Full Article (English):\")\n",
    "    st.write(translated_text)\n",
    "\n",
    "    st.subheader(\"🔍 Summary (English):\")\n",
    "    st.markdown(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce1733ff-0609-4741-a26a-67cc8371fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 23:26:45.270 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-05 23:26:45.636 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\ruhan\\anaconda3\\envs\\tf_env\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-06-05 23:26:45.641 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-05 23:26:45.647 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-05 23:26:45.649 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-05 23:26:45.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-05 23:26:45.657 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-05 23:26:45.660 Session state does not function when running a script without `streamlit run`\n",
      "2025-06-05 23:26:45.662 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-05 23:26:45.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st                          \n",
    "def main():\n",
    "    st.sidebar.title(\"🔗 Hindi News URL Input\")\n",
    "    url = st.sidebar.text_input(\"Paste a Hindi news article URL:\")\n",
    "\n",
    "    if url:\n",
    "        try:\n",
    "            article_data = scrape_news_articles(url)\n",
    "\n",
    "            if article_data['content'] != \"Content not found\":\n",
    "                # Load models\n",
    "                trans_tokenizer, trans_model = load_translation_model()\n",
    "                sum_tokenizer, sum_model = load_summarization_model()\n",
    "\n",
    "                # Translate\n",
    "                translated_title = translate_text(article_data['title'], trans_tokenizer, trans_model)\n",
    "                translated_text = translate_text(article_data['content'], trans_tokenizer, trans_model)\n",
    "\n",
    "                # Summarize\n",
    "                summary = summarize_text(translated_text, sum_tokenizer, sum_model)\n",
    "\n",
    "                # Display\n",
    "                display_news_summary(article_data, translated_title, translated_text, summary)\n",
    "            else:\n",
    "                st.error(\"❌ Could not extract article content.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"⚠️ Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397915ff-743a-4201-92b9-328d510bf9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
